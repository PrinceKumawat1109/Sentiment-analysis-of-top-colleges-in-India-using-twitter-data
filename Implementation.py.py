# -*- coding: utf-8 -*-
"""DL.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/19ZoGkKSSmbAA4DXJnl4iCyf5nBjQV9zb
"""

pip install --upgrade pandas numpy re nltk tensorflow keras

pip install pandas numpy nltk tensorflow keras

pip install keras

import pandas as pd
import numpy as np
import re
import nltk
from nltk.corpus import stopwords
from keras.preprocessing.text import Tokenizer
from keras.utils import pad_sequences
from keras.models import Sequential
from keras.layers import Dense, LSTM, Embedding, Dropout
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score, confusion_matrix

df = pd.read_csv('AIIMS_college.csv', encoding='latin1')

import nltk
nltk.download('stopwords')

import nltk
nltk.download('punkt')

# Data preprocessing
def preprocess(text):
    text = re.sub(r'http\S+', '', text)  # Remove URLs
    text = re.sub(r'@[^\s]+', '', text)  # Remove mentions
    text = re.sub(r'#', '', text)  # Remove hashtags
    text = re.sub(r'[^a-zA-Z\s]', '', text)  # Remove special characters
    text = text.lower()  # Convert text to lowercase
    text = nltk.word_tokenize(text)  # Tokenize text
    text = [word for word in text if word not in stopwords.words('english')]# Remove stopwords
    return ' '.join(text)
df['Tweets'] = df['Tweets'].astype(str)  # convert to string type
df['Tweets'] = df['Tweets'].apply(preprocess)

# Tokenize the text
tokenizer = Tokenizer(num_words=5000, split=' ')
tokenizer.fit_on_texts(df['Tweets'].values)
X = tokenizer.texts_to_sequences(df['Tweets'].values)
X = pad_sequences(X)

# Create target variable
Y = pd.get_dummies(df['Sentiment']).values

# Split the data into training and testing sets
X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2, random_state=42)

# Define the RNN model
model = Sequential()
model.add(Embedding(5000, 128, input_length=X.shape[1]))
model.add(LSTM(128, dropout=0.2, recurrent_dropout=0.2))
model.add(Dense(3, activation='softmax'))

# Compile the model
model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])

# Train the model
model.fit(X_train, Y_train, validation_data=(X_test, Y_test), epochs=5, batch_size=32)

# Evaluate the model
Y_pred = model.predict(X_test)
Y_pred = np.argmax(Y_pred, axis=1)
Y_test = np.argmax(Y_test, axis=1)

print('Accuracy:', accuracy_score(Y_test, Y_pred))
print('Confusion Matrix:\n', confusion_matrix(Y_test, Y_pred))

